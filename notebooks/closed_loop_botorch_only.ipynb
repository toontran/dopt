{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-loop batch, constrained BO in BoTorch with qEI and qNEI\n",
    "\n",
    "In this tutorial, we illustrate how to implement a simple Bayesian Optimization (BO) closed loop in BoTorch.\n",
    "\n",
    "In general, we recommend for a relatively simple setup (like this one) to use Ax, since this will simplify your setup (including the amount of code you need to write) considerably. See the [Using BoTorch with Ax](./custom_botorch_model_in_ax) tutorial.\n",
    "\n",
    "However, you may want to do things that are not easily supported in Ax at this time (like running high-dimensional BO using a VAE+GP model that you jointly train on high-dimensional input data). If you find yourself in such a situation, you will need to write your own optimization loop, as we do in this tutorial.\n",
    "\n",
    "\n",
    "We use the batch Expected Improvement (qEI) and batch Noisy Expected Improvement (qNEI) acquisition functions to optimize a constrained version of the synthetic Hartmann6 test function. The standard problem is\n",
    "\n",
    "$$f(x) = -\\sum_{i=1}^4 \\alpha_i \\exp \\left( -\\sum_{j=1}^6 A_{ij} (x_j - P_{ij})^2  \\right)$$\n",
    "\n",
    "over $x \\in [0,1]^6$ (parameter values can be found in `botorch/test_functions/hartmann6.py`).\n",
    "\n",
    "In real BO applications, the design $x$ can influence multiple metrics in unknown ways, and the decision-maker often wants to optimize one metric without sacrificing another. To illustrate this, we add a synthetic constraint fo the form $\\|x\\|_1 - 3 \\le 0$. Both the objective and the constraint are observed with noise. \n",
    "\n",
    "Since botorch assumes a maximization problem, we will attempt to maximize $-f(x)$ to achieve $\\max_{x} -f(x) = 3.32237$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch import fit_gpytorch_model\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement, qNoisyExpectedImprovement\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "# warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")#(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem setup\n",
    "\n",
    "First, we define the constraint used in the example in `outcome_constraint`. The second function `weighted_obj` is a \"feasibility-weighted objective,\" which returns zero when not feasible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.test_functions.synthetic import Hartmann\n",
    "\n",
    "neg_hartmann6 = Hartmann(negate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "# Helper functions\n",
    "@contextmanager\n",
    "def timer(label):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    yield\n",
    "    print(f\"[Process {label}] elasped in {time.time()-start}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_constraint(X):\n",
    "    \"\"\"L1 constraint; feasible if less than or equal to zero.\"\"\"\n",
    "    return X.sum(dim=-1) - 3\n",
    "\n",
    "def weighted_obj(X):\n",
    "    \"\"\"Feasibility weighted objective; zero if not feasible.\"\"\"\n",
    "    return neg_hartmann6(X) * (outcome_constraint(X) <= 0).type_as(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model initialization\n",
    "\n",
    "We use a `MultiOutputGP` to model the objective (output 0) and the constraint (output 1). We assume known homoskedastic observation noise on both the objective and constraint with standard error $\\sigma = 0.5$. \n",
    "\n",
    "Each component is a `FixedNoiseGP`. The models are initialized with 10 points drawn randomly from $[0,1]^6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models import FixedNoiseGP, ModelListGP, HeteroskedasticSingleTaskGP\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "\n",
    "NOISE_SE = 0.5\n",
    "NOISE_OF_NOISE = 0.1\n",
    "NOISE_FURTHER = 0.2\n",
    "\n",
    "\n",
    "def generate_initial_data(n=10):\n",
    "    train_yvar = []\n",
    "    # generate training data\n",
    "    train_x = torch.rand(n, 6, device=device, dtype=dtype)\n",
    "    print(train_x.shape)\n",
    "    exact_obj = neg_hartmann6(train_x).unsqueeze(-1)  # add output dimension\n",
    "    print(exact_obj.shape)\n",
    "    exact_con = outcome_constraint(train_x).unsqueeze(-1)  # add output dimension\n",
    "    observed_noise = NOISE_SE * torch.randn_like(exact_obj) + NOISE_OF_NOISE * torch.randn_like(exact_obj)\n",
    "    train_obj = exact_obj + observed_noise + NOISE_FURTHER * torch.randn_like(exact_obj)\n",
    "    train_con = exact_con + NOISE_SE * torch.randn_like(exact_con)\n",
    "    best_observed_value = weighted_obj(train_x).max().item()\n",
    "    train_yvar.append(observed_noise)\n",
    "    return train_x, train_obj, train_con, best_observed_value, train_yvar\n",
    "    \n",
    "    \n",
    "def initialize_model(train_x, train_obj, train_con, train_yvar, state_dict=None):\n",
    "    train_yvar_con = torch.tensor(NOISE_SE**2, device=device, dtype=dtype)\n",
    "    train_yvar_obj = torch.cat(train_yvar) ** 2\n",
    "    # define models for objective and constraint\n",
    "    model_obj = HeteroskedasticSingleTaskGP(train_x, train_obj, train_yvar_obj).to(train_x)\n",
    "    model_con = HeteroskedasticSingleTaskGP(train_x, train_con, train_yvar_con.expand_as(train_con)).to(train_x)\n",
    "    # combine into a multi-output GP model\n",
    "    model = ModelListGP(model_obj, model_con)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    # load state dict if it is passed\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a construct to extract the objective and constraint from the GP\n",
    "The methods below take the outputs of the GP and return the objective and the constraint. In general, these can be any `Callable`, but here we simply need to index the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition.objective import ConstrainedMCObjective\n",
    "\n",
    "def obj_callable(Z):\n",
    "    return Z[..., 0]\n",
    "\n",
    "def constraint_callable(Z):\n",
    "    return Z[..., 1]\n",
    "\n",
    "# define a feasibility-weighted objective for optimization\n",
    "constrained_obj = ConstrainedMCObjective(\n",
    "    objective=obj_callable,\n",
    "    constraints=[constraint_callable],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29379725788967226"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper function that performs the essential BO step\n",
    "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. For this example, we'll use a small batch of $q=3$. The function `optimize_acqf` optimizes the $q$ points jointly. A simple initialization heuristic is used to select the 10 restart initial locations from a set of 50 random points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.optim import optimize_acqf\n",
    "from gpytorch.utils.errors import NanError\n",
    "\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "bounds = torch.tensor([[0.0] * 6, [1.0] * 6], device=device, dtype=dtype)\n",
    "\n",
    "# def get_observation()\n",
    "\n",
    "def optimize_acqf_and_get_observation(acq_func):\n",
    "    \"\"\"Optimizes the acquisition function, and returns a new candidate and a noisy observation.\"\"\"\n",
    "    \n",
    "#     with timer(\"Generate candidate\"):\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=10,\n",
    "        raw_samples=500,  # Sample on GP using Sobel sequence\n",
    "        options={\n",
    "            \"batch_limit\": 5,\n",
    "            \"max_iter\": 200,\n",
    "            \"seed\": 0\n",
    "        }\n",
    "    )      \n",
    "            \n",
    "    train_yvar = []\n",
    "    # observe new values <-- new values -> obtained in batches?\n",
    "    new_x = candidates.detach()\n",
    "#     print(new_x.shape)\n",
    "    exact_obj = neg_hartmann6(new_x).unsqueeze(-1)  # add output dimension\n",
    "    exact_con = outcome_constraint(new_x).unsqueeze(-1)  # add output dimension\n",
    "    observed_noise = NOISE_SE * torch.randn_like(exact_obj) + NOISE_OF_NOISE * torch.randn_like(exact_obj)\n",
    "    new_obj = exact_obj + observed_noise + NOISE_FURTHER * torch.randn_like(exact_obj)\n",
    "    new_con = exact_con + NOISE_SE * torch.randn_like(exact_con)\n",
    "    train_yvar.append(observed_noise)\n",
    "    return new_x, new_obj, new_con, train_yvar\n",
    "\n",
    "\n",
    "def update_random_observations(best_random):\n",
    "    \"\"\"Simulates a random policy by taking a the current list of best values observed randomly,\n",
    "    drawing a new random point, observing its value, and updating the list.\n",
    "    \"\"\"\n",
    "    rand_x = torch.rand(BATCH_SIZE, 6)\n",
    "    next_random_best = weighted_obj(rand_x).max().item()\n",
    "    best_random.append(max(best_random[-1], next_random_best))       \n",
    "    return best_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Bayesian Optimization loop with qNEI\n",
    "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps:\n",
    "1. given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$\n",
    "2. observe $f(x)$ for each $x$ in the batch \n",
    "3. update the surrogate model. \n",
    "\n",
    "\n",
    "Just for illustration purposes, we run three trials each of which do `N_BATCH=20` rounds of optimization. The acquisition function is approximated using `MC_SAMPLES=500` samples.\n",
    "\n",
    "*Note*: Running this may take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial  1 of 1 torch.Size([500, 6])\n",
      "torch.Size([500, 1])\n",
      "\n",
      "Batch  1: best_value (random, qEI, qNEI) = 3.16), time = 131.46."
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "\n",
    "N_TRIALS = 1\n",
    "N_BATCH = 1\n",
    "MC_SAMPLES = 500\n",
    "\n",
    "verbose = True\n",
    "\n",
    "best_observed_all_ei, best_observed_all_nei, best_random_all = [], [], []\n",
    "\n",
    "# average over multiple trials\n",
    "for trial in range(1, N_TRIALS + 1):\n",
    "    \n",
    "    print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n",
    "    best_observed_ei, best_observed_nei, best_random = [], [], []\n",
    "    \n",
    "    # call helper functions to generate initial training data and initialize model\n",
    "    train_x_ei, train_obj_ei, train_con_ei, best_observed_value_ei, train_yvar_ei = generate_initial_data(n=500)\n",
    "    mll_ei, model_ei = initialize_model(train_x_ei, train_obj_ei, train_con_ei, train_yvar_ei)\n",
    "    \n",
    "    train_x_nei, train_obj_nei, train_con_nei, train_yvar_nei = train_x_ei, train_obj_ei, train_con_ei, copy(train_yvar_ei)\n",
    "    best_observed_value_nei = best_observed_value_ei\n",
    "    mll_nei, model_nei = initialize_model(train_x_nei, train_obj_nei, train_con_nei, train_yvar_nei)\n",
    "    \n",
    "    best_observed_ei.append(best_observed_value_ei)\n",
    "    best_observed_nei.append(best_observed_value_nei)\n",
    "    best_random.append(best_observed_value_ei)\n",
    "    \n",
    "    # run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "    for iteration in range(1, N_BATCH + 1):    \n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # fit the models\n",
    "#         fit_gpytorch_model(mll_ei)\n",
    "        fit_gpytorch_model(mll_nei)\n",
    "        \n",
    "        # define the qEI and qNEI acquisition modules using a QMC sampler\n",
    "        qmc_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES, seed=0)\n",
    "        \n",
    "        # for best_f, we use the best observed noisy values as an approximation\n",
    "#         qEI = qExpectedImprovement(\n",
    "#             model=model_ei, \n",
    "#             best_f=(train_obj_ei * (train_con_ei <= 0).to(train_obj_ei)).max(),\n",
    "#             sampler=qmc_sampler, \n",
    "#             objective=constrained_obj,\n",
    "#         )\n",
    "        \n",
    "        qNEI = qNoisyExpectedImprovement(\n",
    "            model=model_nei, \n",
    "            X_baseline=train_x_nei,\n",
    "            sampler=qmc_sampler, \n",
    "            objective=constrained_obj,\n",
    "        )\n",
    "        \n",
    "        # optimize and get new observation\n",
    "#         new_x_ei, new_obj_ei, new_con_ei, new_yvar_ei = optimize_acqf_and_get_observation(qEI)\n",
    "        new_x_nei, new_obj_nei, new_con_nei, new_yvar_nei = optimize_acqf_and_get_observation(qNEI)\n",
    "                \n",
    "        # update training points\n",
    "#         train_x_ei = torch.cat([train_x_ei, new_x_ei])\n",
    "#         train_obj_ei = torch.cat([train_obj_ei, new_obj_ei])\n",
    "#         train_con_ei = torch.cat([train_con_ei, new_con_ei])\n",
    "#         train_yvar_ei.extend(new_yvar_ei)\n",
    "\n",
    "        train_x_nei = torch.cat([train_x_nei, new_x_nei])\n",
    "        train_obj_nei = torch.cat([train_obj_nei, new_obj_nei])\n",
    "        train_con_nei = torch.cat([train_con_nei, new_con_nei])\n",
    "        train_yvar_nei.extend(new_yvar_nei)\n",
    "\n",
    "        # update progress\n",
    "#         best_random = update_random_observations(best_random)\n",
    "#         best_value_ei = weighted_obj(train_x_ei).max().item()\n",
    "        best_value_nei = weighted_obj(train_x_nei).max().item()\n",
    "#         best_observed_ei.append(best_value_ei)\n",
    "        best_observed_nei.append(best_value_nei)\n",
    "\n",
    "        # reinitialize the models so they are ready for fitting on next iteration\n",
    "        # use the current state dict to speed up fitting\n",
    "#         mll_ei, model_ei = initialize_model(\n",
    "#             train_x_ei, \n",
    "#             train_obj_ei, \n",
    "#             train_con_ei,\n",
    "#             train_yvar_ei,\n",
    "#             model_ei.state_dict(),\n",
    "#         )\n",
    "        mll_nei, model_nei = initialize_model(\n",
    "            train_x_nei, \n",
    "            train_obj_nei, \n",
    "            train_con_nei,\n",
    "            train_yvar_nei,\n",
    "            model_nei.state_dict(),\n",
    "        )\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\nBatch {iteration:>2}: best_value (random, qEI, qNEI) = \"\n",
    "                #f\"({max(best_random):>4.2f}, {best_value_ei:>4.2f}, \n",
    "                f\"{best_value_nei:>4.2f}), \"\n",
    "                f\"time = {t1-t0:>4.2f}.\", end=\"\"\n",
    "            )\n",
    "        else:\n",
    "            print(\".\", end=\"\")\n",
    "   \n",
    "    best_observed_all_ei.append(best_observed_ei)\n",
    "    best_observed_all_nei.append(best_observed_nei)\n",
    "    best_random_all.append(best_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9081, 0.4792, 0.4013, 0.4520, 0.8302, 0.6463],\n",
       "         [0.6200, 0.9347, 0.2195, 0.7131, 0.6599, 0.3554],\n",
       "         [0.1725, 0.1999, 0.4361, 0.1504, 0.1866, 0.7859]], dtype=torch.float64),\n",
       " tensor([[0.2415],\n",
       "         [0.3799],\n",
       "         [1.6446]], dtype=torch.float64),\n",
       " tensor([[ 0.9606],\n",
       "         [ 0.6880],\n",
       "         [-1.3711]], dtype=torch.float64),\n",
       " [tensor([[ 0.2367],\n",
       "          [-0.2031],\n",
       "          [-0.2133]], dtype=torch.float64)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mll_ei, model_ei = initialize_model(\n",
    "    train_x_ei, \n",
    "    train_obj_ei, \n",
    "    train_con_ei,\n",
    "    train_yvar_ei,\n",
    "    model_ei.state_dict(),\n",
    ")\n",
    "\n",
    "# fit the models\n",
    "# fit_gpytorch_model(mll_ei)\n",
    "\n",
    "# define the qEI and qNEI acquisition modules using a QMC sampler\n",
    "qmc_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES, seed=0)\n",
    "\n",
    "# for best_f, we use the best observed noisy values as an approximation\n",
    "qEI = qExpectedImprovement(\n",
    "    model=model_ei, \n",
    "    best_f=(train_obj_ei * (train_con_ei <= 0).to(train_obj_ei)).max(),\n",
    "    sampler=qmc_sampler, \n",
    "    objective=constrained_obj,\n",
    ")\n",
    "\n",
    "# optimize and get new observation\n",
    "optimize_acqf_and_get_observation(qEI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([3, 6]), y: torch.Size([3, 1]), var: torch.Size([3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x1': 0.47448268759621093,\n",
       " 'x2': 0.06570019846549614,\n",
       " 'x3': 0.1983489942378863,\n",
       " 'x4': 0.19961448365460774,\n",
       " 'x5': 0.2908090385295882,\n",
       " 'x6': 1.0}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for the neioptimizer\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "\n",
    "def initialize_model(candidate, training_result, state_dict=None):\n",
    "    global train_x, train_obj, train_var\n",
    "    # Group candidates, objectives and variances and put into a torch Tensor\n",
    "    train_x, train_obj, train_var = [], [], []\n",
    "    for o in observations:\n",
    "        train_x.append(list(o[\"candidate\"].values()))\n",
    "        train_obj.append(o[\"result\"][0])\n",
    "        train_var.append(o[\"result\"][1])\n",
    "    train_x.append(list(candidate.values()))\n",
    "    train_obj.append(training_result[0])\n",
    "    train_var.append(training_result[1])\n",
    "    train_x = torch.tensor(train_x, device=device, dtype=dtype)\n",
    "    train_obj = torch.tensor(train_obj, device=device, dtype=dtype).unsqueeze(-1)\n",
    "    train_var = torch.tensor(train_var, device=device, dtype=dtype).unsqueeze(-1)\n",
    "\n",
    "    print(f\"X: {train_x.shape}, y: {train_obj.shape}, var: {train_var.shape}\")\n",
    "    # define models for objective and constraint\n",
    "    model = HeteroskedasticSingleTaskGP(train_x, train_obj, train_var**2).to(train_x)\n",
    "    # combine into a multi-output GP model\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    # load state dict if it is passed\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return mll, model\n",
    "\n",
    "observations = [\n",
    "        {\"candidate\": {\"x1\":0.36589372, \"x2\":0.16217732, \"x3\":0.3572523, \"x4\":0.08772629, \"x5\":0.47287726, \"x6\":0.9107906},\n",
    "         \"result\": (1.0175396, -0.10288623)},\n",
    "        {\"candidate\": {\"x1\":0.2034043, \"x2\":0.5584821, \"x3\":0.78182673, \"x4\":0.07152402, \"x5\":0.88407385, \"x6\":0.46330553},\n",
    "        'result': (-0.14934252, -0.23847157)}\n",
    "]\n",
    "trainer_info = {'result': (0.5329527, 0.15520538)}\n",
    "candidate = {\"x1\":0.5, \"x2\":0.5, \"x3\":0.5, \"x4\":0.5, \"x5\":0.5, \"x6\":0.6}\n",
    "bounds = {\"x1\": (0, 1), \"x2\": (0, 1), \"x3\": (0, 1), \"x4\": (0, 1), \"x5\": (0, 1), \"x6\": (0, 1)}\n",
    "\n",
    "\n",
    "lower_bounds = [bound[0] for bound in bounds.values()]\n",
    "upper_bounds = [bound[1] for bound in bounds.values()]\n",
    "torch_bounds = torch.tensor([lower_bounds, upper_bounds], device=device, dtype=dtype)\n",
    "mll, model = initialize_model(candidate, trainer_info[\"result\"])\n",
    "fit_gpytorch_model(mll)\n",
    "qmc_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES, seed=0)\n",
    "qNEI = qNoisyExpectedImprovement(\n",
    "    model=model, \n",
    "    X_baseline=train_x,\n",
    "    sampler=qmc_sampler, \n",
    ")\n",
    "torch_candidate, _ = optimize_acqf(\n",
    "    acq_function=qNEI,\n",
    "    bounds=torch_bounds,\n",
    "    q=1,\n",
    "    num_restarts=10,\n",
    "    raw_samples=500,  # Sample on GP using Sobel sequence\n",
    "    options={\n",
    "        \"batch_limit\": 5,\n",
    "        \"max_iter\": 200,\n",
    "        \"seed\": 0\n",
    "    }\n",
    ")    \n",
    "\n",
    "candidate = {}\n",
    "for i, key in enumerate(bounds.keys()):\n",
    "    candidate[key] = torch_candidate.cpu().numpy()[0][i]\n",
    "candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': 0.4745468242989644,\n",
       " 'x2': 0.06562127492160173,\n",
       " 'x3': 0.19841943193999256,\n",
       " 'x4': 0.1996072423922306,\n",
       " 'x5': 0.29071384478280604,\n",
       " 'x6': 1.0}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import *\n",
    "\n",
    "observations = [\n",
    "        {\"candidate\": {\"x1\":0.36589372, \"x2\":0.16217732, \"x3\":0.3572523, \"x4\":0.08772629, \"x5\":0.47287726, \"x6\":0.9107906},\n",
    "         \"result\": (1.0175396, -0.10288623)},\n",
    "        {\"candidate\": {\"x1\":0.2034043, \"x2\":0.5584821, \"x3\":0.78182673, \"x4\":0.07152402, \"x5\":0.88407385, \"x6\":0.46330553},\n",
    "        'result': (-0.14934252, -0.23847157)}\n",
    "]\n",
    "trainer_info = {'result': (0.5329527, 0.15520538)}\n",
    "candidate = {\"x1\":0.5, \"x2\":0.5, \"x3\":0.5, \"x4\":0.5, \"x5\":0.5, \"x6\":0.6}\n",
    "bounds = {\"x1\": (0, 1), \"x2\": (0, 1), \"x3\": (0, 1), \"x4\": (0, 1), \"x5\": (0, 1), \"x6\": (0, 1)}\n",
    "\n",
    "def initialize_model(candidate, training_result, state_dict=None):\n",
    "    r\"\"\" TODO\n",
    "\n",
    "    :param candidate: \n",
    "    \"\"\"\n",
    "\n",
    "    # Group candidates, objectives and variances from observations \n",
    "    # and put into a torch Tensor\n",
    "    train_x, train_obj, train_var = [], [], []\n",
    "    for o in observations:\n",
    "        train_x.append(list(o[\"candidate\"].values()))\n",
    "        train_obj.append(o[\"result\"][0])\n",
    "        train_var.append(o[\"result\"][1])\n",
    "    train_x.append(list(candidate.values()))\n",
    "    train_obj.append(training_result[0])\n",
    "    train_var.append(training_result[1])\n",
    "#     print(train_x)\n",
    "    train_x = torch.tensor(train_x, device=device, dtype=dtype)\n",
    "    train_obj = torch.tensor(train_obj, device=device, dtype=dtype).unsqueeze(-1)\n",
    "    train_var = torch.tensor(train_var, device=device, dtype=dtype).unsqueeze(-1)\n",
    "\n",
    "    # define models for objective and constraint\n",
    "    model = HeteroskedasticSingleTaskGP(train_x, train_obj, train_var**2).to(train_x)\n",
    "    # combine into a multi-output GP model\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    # load state dict if it is passed\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return mll, model\n",
    "\n",
    "# TODO: Change model initialization upon NanError thrown by cholesky decomposition\n",
    "def generate_candidate(candidate: Dict[str, Any], trainer_info: Union[Dict, None]) \\\n",
    "        -> Dict[str, Any]:\n",
    "    if trainer_info is None and len(observations) == 0:\n",
    "        # Generate a random candidate to startup the optimizing process\n",
    "        return self._generate_random_candidate()\n",
    "    elif trainer_info is None and len(observations) > 0:\n",
    "        # TODO: Need to handle this case\n",
    "        pass\n",
    "    else:\n",
    "        mll, model = initialize_model(candidate, trainer_info[\"result\"])\n",
    "        fit_gpytorch_model(mll)\n",
    "        qmc_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES, seed=0)\n",
    "        qNEI = qNoisyExpectedImprovement(\n",
    "            model=model, \n",
    "            X_baseline=torch.tensor([list(o[\"candidate\"].values()) for o in observations], device=device, dtype=dtype),\n",
    "            sampler=qmc_sampler, \n",
    "        )\n",
    "\n",
    "        # Torch based bounds\n",
    "        lower_bounds = [bound[0] for bound in bounds.values()]\n",
    "        upper_bounds = [bound[1] for bound in bounds.values()]\n",
    "        bounds_torch = torch.tensor([lower_bounds, upper_bounds], device=device, dtype=dtype)\n",
    "        torch_candidate, _ = optimize_acqf(\n",
    "            acq_function=qNEI,\n",
    "            bounds=bounds_torch,\n",
    "            q=1,              # Generate only one candidate at a time\n",
    "            num_restarts=10,  # ???\n",
    "            raw_samples=500,  # Sample on GP using Sobel sequence\n",
    "            options={\n",
    "                \"batch_limit\": 5,\n",
    "                \"max_iter\": 200,\n",
    "                \"seed\": 0\n",
    "            }\n",
    "        )  \n",
    "\n",
    "        candidate = {}\n",
    "        for i, key in enumerate(bounds.keys()):\n",
    "            candidate[key] = torch_candidate.cpu().numpy()[0][i]\n",
    "\n",
    "        return candidate\n",
    "    \n",
    "generate_candidate(candidate, trainer_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'candidate': {'x1': 0.36589372,\n",
       "   'x2': 0.16217732,\n",
       "   'x3': 0.3572523,\n",
       "   'x4': 0.08772629,\n",
       "   'x5': 0.47287726,\n",
       "   'x6': 0.9107906},\n",
       "  'result': (1.0175396, -0.10288623)},\n",
       " {'candidate': {'x1': 0.2034043,\n",
       "   'x2': 0.5584821,\n",
       "   'x3': 0.78182673,\n",
       "   'x4': 0.07152402,\n",
       "   'x5': 0.88407385,\n",
       "   'x6': 0.46330553},\n",
       "  'result': (-0.14934252, -0.23847157)}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[list(o[\"candidate\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2034043  0.5584821  0.78182673 0.07152402 0.88407385 0.46330553]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results': (-0.14934252, -0.23847157)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "candidate = {\"x1\":0.5, \"x2\":0.5, \"x3\":0.5, \"x4\":0.5, \"x5\":0.5, \"x6\":0.6}\n",
    "\n",
    "def get_observation(candidate: Dict[str, Any]) \\\n",
    "            -> Dict[str, Any]:\n",
    "        r\"\"\" Get observation by plugging the candidate into objective function.\n",
    "        This method is made abstract to easier modify the objective function\n",
    "        to run on different platforms.\n",
    "\n",
    "        :param candidate:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        objective_function = Hartmann(negate=True)\n",
    "        train_x = torch.rand(6, device=device)\n",
    "        exact_obj = objective_function(train_x).unsqueeze(-1)  # add output dimension\n",
    "        # Add noise to the objective function\n",
    "        observed_noise = NOISE_SE * torch.randn_like(exact_obj) + NOISE_OF_NOISE * torch.randn_like(exact_obj)\n",
    "        train_obj = exact_obj + observed_noise + NOISE_FURTHER * torch.randn_like(exact_obj)\n",
    "        print(train_x.cpu().numpy())\n",
    "        return {\"results\": (train_obj.cpu().numpy()[0], observed_noise.cpu().numpy()[0])}\n",
    "    \n",
    "get_observation(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results\n",
    "The plot below shows the best objective value observed at each step of the optimization for each of the algorithms. The confidence intervals represent the variance at that step in the optimization across the trial runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aead6978682548af8a4713f9f6789932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6e210249852a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_rnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_rnd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_ei\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"qEI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_nei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_nei\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"qNEI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/remote/anaconda-3.7-2020-01-14/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/remote/anaconda-3.7-2020-01-14/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36merrorbar\u001b[0;34m(self, x, y, yerr, xerr, fmt, ecolor, elinewidth, capsize, barsabove, lolims, uplims, xlolims, xuplims, errorevery, capthick, **kwargs)\u001b[0m\n\u001b[1;32m   3426\u001b[0m             \u001b[0mnoylims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlolims\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0muplims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnoylims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoylims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3428\u001b[0;31m                 \u001b[0mxo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxywhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoylims\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0meverymask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3429\u001b[0m                 \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxywhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoylims\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0meverymask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m                 \u001b[0mbarcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meb_lines_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/remote/anaconda-3.7-2020-01-14/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mxywhere\u001b[0;34m(xs, ys, mask)\u001b[0m\n\u001b[1;32m   3329\u001b[0m             \u001b[0mys\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3330\u001b[0m             \"\"\"\n\u001b[0;32m-> 3331\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3332\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3333\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthisx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthisx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "GLOBAL_MAXIMUM = neg_hartmann6.optimal_value\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "def ci(y):\n",
    "    return 1.96 * y.std(axis=0) / np.sqrt(N_TRIALS)\n",
    "\n",
    "iters = np.arange(N_BATCH + 1) * BATCH_SIZE\n",
    "y_ei = np.asarray(best_observed_all_ei)\n",
    "y_nei = np.asarray(best_observed_all_nei)\n",
    "y_rnd = np.asarray(best_random_all)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.errorbar(iters, y_rnd.mean(axis=0), yerr=ci(y_rnd), label=\"random\", linewidth=1.5)\n",
    "ax.errorbar(iters, y_ei.mean(axis=0), yerr=ci(y_ei), label=\"qEI\", linewidth=1.5)\n",
    "ax.errorbar(iters, y_nei.mean(axis=0), yerr=ci(y_nei), label=\"qNEI\", linewidth=1.5)\n",
    "plt.plot([0, N_BATCH * BATCH_SIZE], [GLOBAL_MAXIMUM] * 2, 'k', label=\"true best bjective\", linewidth=2)\n",
    "ax.set_ylim(bottom=0.5)\n",
    "ax.set(xlabel='number of observations (beyond initial points)', ylabel='best objective value')\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
